{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a98d661-866e-4e0d-b238-ce660ecc8ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/jet/home/cpestell/lab3-1')\n",
    "from ridge_utils.DataSequence import DataSequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcd45bef-1a33-4fdb-a8e2-764e55a1f90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/cpestell/.conda/envs/env_214/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..') \n",
    "from ridge_utils.DataSequence import DataSequence\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "from encoder import Encoder\n",
    "from data import TextDataset\n",
    "from train_encoder import train_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c65959-9748-44dc-998f-e6ad3bf6c8d1",
   "metadata": {},
   "source": [
    "# Train Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21a79ca3-eab4-4271-98f4-fadb3c586e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_61984/3662475046.py:6: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  raw_text = pickle.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 109 podcast stories\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'bert-base-uncased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-base-uncased' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     all_sentences\u001b[38;5;241m.\u001b[39mextend(story\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Tokenizer & Dataset\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TextDataset(all_sentences, tokenizer, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m     18\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/env_214/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:2046\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[1;32m   2044\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[1;32m   2045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 2046\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2047\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2048\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2049\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2050\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2051\u001b[0m     )\n\u001b[1;32m   2053\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'bert-base-uncased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-base-uncased' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "# Load the podcast text data\n",
    "data_path = \"/ocean/projects/mth240012p/shared/data\"\n",
    "raw_text_path = os.path.join(data_path, \"raw_text.pkl\")\n",
    "\n",
    "with open(raw_text_path, \"rb\") as f:\n",
    "    raw_text = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(raw_text)} podcast stories\")\n",
    "\n",
    "# Flatten all the words from all stories into a single list\n",
    "all_sentences = []\n",
    "for story in raw_text.values():\n",
    "    all_sentences.extend(story.data)\n",
    "\n",
    "# Tokenizer & Dataset\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = TextDataset(all_sentences, tokenizer, max_len=32)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create encoder model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "encoder = Encoder(vocab_size=tokenizer.vocab_size)\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "# Train the model using masked language modeling\n",
    "train_bert(\n",
    "    model=encoder,\n",
    "    dataloader=dataloader,\n",
    "    tokenizer=tokenizer,\n",
    "    epochs=20, # we can change this \n",
    "    lr=5e-4, # same as well \n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Save pretrained encoder weights\n",
    "torch.save(encoder.state_dict(), \"pretrained_encoder.pt\")\n",
    "print(\"Pretrained encoder saved as pretrained_encoder.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21400b10-1f07-4bca-868b-314cf6252bbd",
   "metadata": {},
   "source": [
    "# Generate All Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f1b1eb-bc15-4944-aae9-793014eea372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle, os\n",
    "from transformers import BertTokenizerFast\n",
    "from encoder import Encoder\n",
    "from preprocessing import downsample_word_vectors, make_delayed\n",
    "\n",
    "# Setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"/jet/home/cpestell/bert-base-uncased\")  # your local copy\n",
    "encoder = Encoder(vocab_size=tokenizer.vocab_size)\n",
    "encoder.load_state_dict(torch.load(\"pretrained_encoder.pt\", map_location=device))\n",
    "encoder.to(device)\n",
    "encoder.eval()\n",
    "\n",
    "# Load raw text\n",
    "data_path = \"/ocean/projects/mth240012p/shared/data\"\n",
    "with open(os.path.join(data_path, \"raw_text.pkl\"), \"rb\") as f:\n",
    "    raw_text = pickle.load(f)\n",
    "\n",
    "story_names = list(raw_text.keys())\n",
    "\n",
    "# Step 1: Generate encoder embeddings\n",
    "encoder_vectors = {}\n",
    "for story in story_names:\n",
    "    words = raw_text[story].data\n",
    "    inputs = tokenizer(words, padding=True, truncation=True, return_tensors=\"pt\", max_length=64)\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    token_type_ids = inputs[\"token_type_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden = encoder(input_ids, token_type_ids, attention_mask)\n",
    "\n",
    "    # Mean over tokens (seq_len)\n",
    "    sentence_embeddings = hidden.mean(dim=1).cpu().numpy()\n",
    "    encoder_vectors[story] = sentence_embeddings\n",
    "    print(f\"{story}: Encoder shape = {encoder_vectors[story].shape}\")\n",
    "\n",
    "# Step 2: Downsample\n",
    "wordseqs = raw_text\n",
    "downsampled_encoder = downsample_word_vectors(story_names, encoder_vectors, wordseqs)\n",
    "\n",
    "# Step 3: Trim\n",
    "X_encoder_trimmed = {}\n",
    "for story in downsampled_encoder:\n",
    "    X_encoder_trimmed[story] = downsampled_encoder[story][5:-10, :]\n",
    "    print(f\"Trimmed shape for {story}:\", X_encoder_trimmed[story].shape)\n",
    "\n",
    "# Step 4: Create lagged features\n",
    "X_encoder_lagged = {}\n",
    "for story in X_encoder_trimmed:\n",
    "    X_encoder_lagged[story] = make_delayed(X_encoder_trimmed[story], delays=range(1, 5))\n",
    "    print(f\"Lagged shape for {story}:\", X_encoder_lagged[story].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4113160d-c96e-4372-87a6-b96d8ac4fdef",
   "metadata": {},
   "source": [
    "# Modelling and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127fcd73-3294-4de8-8ee6-7ddc00f85175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --------- Setup ---------\n",
    "subject2_path = os.path.join(data_path, \"subject2\")\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for story in train_stories:\n",
    "    if story in X_encoder_lagged:\n",
    "        bold_path = os.path.join(subject2_path, f\"{story}.npy\")\n",
    "        if os.path.exists(bold_path):\n",
    "            Y = np.load(bold_path)\n",
    "            X = X_encoder_lagged[story]\n",
    "            min_len = min(X.shape[0], Y.shape[0])\n",
    "            X_train.append(X[:min_len])\n",
    "            Y_train.append(Y[:min_len])\n",
    "        else:\n",
    "            print(f\"Missing Y file for: {story}\")\n",
    "\n",
    "X_train = np.vstack(X_train)\n",
    "Y_train = np.vstack(Y_train)\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"Y_train:\", Y_train.shape)\n",
    "\n",
    "# --------- Clean NaNs ---------\n",
    "mask = ~np.isnan(Y_train).any(axis=1)\n",
    "X_train_clean = X_train[mask]\n",
    "Y_train_clean = Y_train[mask]\n",
    "\n",
    "# --------- Cross Validation ---------\n",
    "ridge = Ridge(alpha=100.0)\n",
    "cv_scores = cross_val_score(ridge, X_train_clean, Y_train_clean, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean cross-validation score: {np.mean(cv_scores):.4f}\")\n",
    "\n",
    "# --------- Train Model ---------\n",
    "ridge.fit(X_train_clean, Y_train_clean)\n",
    "\n",
    "# --------- Evaluate on Test Set ---------\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "for story in test_stories:\n",
    "    if story in X_encoder_lagged:\n",
    "        bold_path = os.path.join(subject2_path, f\"{story}.npy\")\n",
    "        if os.path.exists(bold_path):\n",
    "            Y = np.load(bold_path)\n",
    "            X = X_encoder_lagged[story]\n",
    "            min_len = min(X.shape[0], Y.shape[0])\n",
    "            X_test.append(X[:min_len])\n",
    "            Y_test.append(Y[:min_len])\n",
    "        else:\n",
    "            print(f\"Missing Y file for: {story}\")\n",
    "\n",
    "X_test = np.vstack(X_test)\n",
    "Y_test = np.vstack(Y_test)\n",
    "\n",
    "# Clean NaNs from test\n",
    "mask = ~np.isnan(Y_test).any(axis=1)\n",
    "X_test_clean = X_test[mask]\n",
    "Y_test_clean = Y_test[mask]\n",
    "\n",
    "# Predict\n",
    "Y_pred = ridge.predict(X_test_clean)\n",
    "\n",
    "# --------- Compute Correlation Coefficients (CCs) ---------\n",
    "def compute_voxel_ccs(y_true, y_pred):\n",
    "    ccs = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        try:\n",
    "            cc, _ = pearsonr(y_true[:, i], y_pred[:, i])\n",
    "        except:\n",
    "            cc = np.nan\n",
    "        ccs.append(cc)\n",
    "    return np.array(ccs)\n",
    "\n",
    "ccs_encoder = compute_voxel_ccs(Y_test_clean, Y_pred)\n",
    "\n",
    "print(f\"Mean CC: {np.mean(ccs_encoder):.4f}\")\n",
    "print(f\"Median CC: {np.median(ccs_encoder):.4f}\")\n",
    "print(f\"Top 1% CC: {np.percentile(ccs_encoder, 99):.4f}\")\n",
    "print(f\"Top 5% CC: {np.percentile(ccs_encoder, 95):.4f}\")\n",
    "\n",
    "# --------- Visualization ---------\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.histplot(ccs_encoder[~np.isnan(ccs_encoder)], bins=100, kde=True, color=\"#1f77b4\", edgecolor=\"black\", stat=\"count\")\n",
    "plt.axvline(np.mean(ccs_encoder), color='red', linestyle='--', label=f\"Mean: {np.mean(ccs_encoder):.3f}\")\n",
    "plt.axvline(np.median(ccs_encoder), color='green', linestyle='-.', label=f\"Median: {np.median(ccs_encoder):.3f}\")\n",
    "plt.title(\"Encoder Correlation Coefficient (CC) Distribution (5000 Voxels)\", fontsize=16, weight=\"bold\")\n",
    "plt.xlabel(\"Correlation Coefficient\", fontsize=13)\n",
    "plt.ylabel(\"Number of Voxels\", fontsize=13)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "save_dir = os.path.join(\"..\", \"results\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "plot_path = os.path.join(save_dir, \"cc_distribution_encoder_subject2_5000vox.png\")\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plot saved to: {plot_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_214",
   "language": "python",
   "name": "env_214"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
