{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcd45bef-1a33-4fdb-a8e2-764e55a1f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') \n",
    "from ridge_utils.DataSequence import DataSequence\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "from encoder import Encoder\n",
    "from data import TextDataset\n",
    "from train_encoder import train_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c65959-9748-44dc-998f-e6ad3bf6c8d1",
   "metadata": {},
   "source": [
    "# Train Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a79ca3-eab4-4271-98f4-fadb3c586e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 109 podcast stories\n",
      "Training model with hidden_size=128, num_layers=2\n",
      "Epoch [1/15], Train Loss: 2.3667, Val Loss: 2.2113\n",
      "Epoch [2/15], Train Loss: 2.1728, Val Loss: 2.1585\n",
      "Epoch [3/15], Train Loss: 2.1292, Val Loss: 2.2192\n",
      "Epoch [4/15], Train Loss: 2.1172, Val Loss: 2.1168\n",
      "Epoch [5/15], Train Loss: 2.1089, Val Loss: 2.1219\n",
      "Epoch [6/15], Train Loss: 2.0830, Val Loss: 2.0350\n",
      "Epoch [7/15], Train Loss: 2.0747, Val Loss: 2.0338\n",
      "Epoch [8/15], Train Loss: 2.0671, Val Loss: 2.1197\n",
      "Epoch [9/15], Train Loss: 2.0488, Val Loss: 2.0863\n",
      "Epoch [10/15], Train Loss: 2.0568, Val Loss: 2.1351\n",
      "Epoch [11/15], Train Loss: 2.0822, Val Loss: 2.0783\n",
      "Epoch [12/15], Train Loss: 2.0796, Val Loss: 2.1745\n",
      "Epoch [13/15], Train Loss: 2.0771, Val Loss: 2.1639\n",
      "Epoch [14/15], Train Loss: 2.0895, Val Loss: 2.1893\n",
      "Epoch [15/15], Train Loss: 2.1082, Val Loss: 2.1990\n",
      "Training model with hidden_size=128, num_layers=4\n",
      "Epoch [1/15], Train Loss: 2.3595, Val Loss: 2.1906\n",
      "Epoch [2/15], Train Loss: 2.1851, Val Loss: 2.1514\n",
      "Epoch [3/15], Train Loss: 2.1578, Val Loss: 2.1551\n",
      "Epoch [4/15], Train Loss: 2.1174, Val Loss: 2.1086\n",
      "Epoch [5/15], Train Loss: 2.1023, Val Loss: 2.0981\n",
      "Epoch [6/15], Train Loss: 2.0956, Val Loss: 2.1128\n",
      "Epoch [7/15], Train Loss: 2.1093, Val Loss: 2.1060\n",
      "Epoch [8/15], Train Loss: 2.0833, Val Loss: 2.0682\n",
      "Epoch [9/15], Train Loss: 2.0576, Val Loss: 2.1076\n",
      "Epoch [10/15], Train Loss: 2.0887, Val Loss: 2.1079\n",
      "Epoch [11/15], Train Loss: 2.0933, Val Loss: 2.1165\n",
      "Epoch [12/15], Train Loss: 2.1019, Val Loss: 2.1954\n",
      "Epoch [13/15], Train Loss: 2.0938, Val Loss: 2.1298\n",
      "Epoch [14/15], Train Loss: 2.1000, Val Loss: 2.1920\n",
      "Epoch [15/15], Train Loss: 2.0968, Val Loss: 2.2332\n",
      "Training model with hidden_size=128, num_layers=6\n",
      "Epoch [1/15], Train Loss: 2.3980, Val Loss: 2.3201\n",
      "Epoch [2/15], Train Loss: 2.2692, Val Loss: 2.2997\n",
      "Epoch [3/15], Train Loss: 2.2358, Val Loss: 2.2729\n",
      "Epoch [4/15], Train Loss: 2.2437, Val Loss: 2.2771\n",
      "Epoch [5/15], Train Loss: 2.2871, Val Loss: 2.3201\n",
      "Epoch [6/15], Train Loss: 2.3244, Val Loss: 2.5854\n",
      "Epoch [7/15], Train Loss: 2.3404, Val Loss: 2.3099\n",
      "Epoch [8/15], Train Loss: 2.2387, Val Loss: 3.0892\n",
      "Epoch [9/15], Train Loss: 2.2441, Val Loss: 2.1973\n",
      "Epoch [10/15], Train Loss: 2.2258, Val Loss: 2.2580\n",
      "Epoch [11/15], Train Loss: 2.4637, Val Loss: 2.2254\n",
      "Epoch [12/15], Train Loss: 2.2338, Val Loss: 2.3300\n",
      "Epoch [13/15], Train Loss: 2.2292, Val Loss: 2.2388\n",
      "Epoch [14/15], Train Loss: 2.2487, Val Loss: 2.3256\n",
      "Epoch [15/15], Train Loss: 2.2438, Val Loss: 2.2144\n",
      "Training model with hidden_size=256, num_layers=2\n",
      "Epoch [1/15], Train Loss: 2.3865, Val Loss: 2.3083\n",
      "Epoch [2/15], Train Loss: 2.2284, Val Loss: 2.2231\n",
      "Epoch [3/15], Train Loss: 2.1613, Val Loss: 2.1211\n",
      "Epoch [4/15], Train Loss: 2.1216, Val Loss: 2.1447\n",
      "Epoch [5/15], Train Loss: 2.1030, Val Loss: 2.1469\n",
      "Epoch [6/15], Train Loss: 2.1088, Val Loss: 2.1573\n",
      "Epoch [7/15], Train Loss: 2.0872, Val Loss: 2.0818\n",
      "Epoch [8/15], Train Loss: 2.0829, Val Loss: 2.1501\n",
      "Epoch [9/15], Train Loss: 2.1060, Val Loss: 2.1443\n",
      "Epoch [10/15], Train Loss: 2.0787, Val Loss: 2.1172\n",
      "Epoch [11/15], Train Loss: 2.0995, Val Loss: 2.1185\n",
      "Epoch [12/15], Train Loss: nan, Val Loss: 2.1884\n",
      "Epoch [13/15], Train Loss: 2.1339, Val Loss: 2.1844\n",
      "Epoch [14/15], Train Loss: 2.0890, Val Loss: 2.2793\n",
      "Epoch [15/15], Train Loss: 2.1020, Val Loss: 2.1875\n",
      "Training model with hidden_size=256, num_layers=4\n",
      "Epoch [1/15], Train Loss: 2.4436, Val Loss: 2.3025\n"
     ]
    }
   ],
   "source": [
    "# Load the podcast text data\n",
    "data_path = \"/ocean/projects/mth240012p/shared/data\"\n",
    "raw_text_path = os.path.join(data_path, \"raw_text.pkl\")\n",
    "\n",
    "with open(raw_text_path, \"rb\") as f:\n",
    "    raw_text = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(raw_text)} podcast stories\")\n",
    "\n",
    "# Flatten all the words from all stories into a single list\n",
    "all_sentences = []\n",
    "for story in raw_text.values():\n",
    "    all_sentences.extend(story.data)\n",
    "\n",
    "# Tokenizer & Dataset\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = TextDataset(all_sentences, tokenizer, max_len=32)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create encoder model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# encoder = Encoder(vocab_size=tokenizer.vocab_size)\n",
    "# encoder = encoder.to(device)\n",
    "\n",
    "import torch\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fixed settings\n",
    "lr = 3e-4\n",
    "epochs = 15\n",
    "batch_size = 32\n",
    "num_heads = 4\n",
    "intermediate_size = 512\n",
    "\n",
    "# Things to tune\n",
    "hidden_sizes = [128, 256, 512]\n",
    "num_layers_list = [2, 4, 6]\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# Loop over all combinations\n",
    "for hidden_size, num_layers in itertools.product(hidden_sizes, num_layers_list):\n",
    "    print(f\"Training model with hidden_size={hidden_size}, num_layers={num_layers}\")\n",
    "\n",
    "    # Initialize new encoder\n",
    "    encoder = Encoder(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=num_layers,\n",
    "        intermediate_size=intermediate_size,\n",
    "        max_len=512\n",
    "    )\n",
    "    encoder = encoder.to(device)\n",
    "\n",
    "    # Train and get losses\n",
    "    train_losses, val_losses = train_bert(\n",
    "        model=encoder,\n",
    "        dataloader=dataloader,\n",
    "        tokenizer=tokenizer,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Save\n",
    "    results[(hidden_size, num_layers)] = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses\n",
    "    }\n",
    "\n",
    "print(\"Finished hyperparameter tuning!\")\n",
    "\n",
    "\n",
    "# Plot validation loss curves for all runs\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for (hidden_size, num_layers), losses in results.items():\n",
    "    val_losses = losses['val_losses']\n",
    "    label = f\"hs={hidden_size}, nl={num_layers}\"\n",
    "    plt.plot(range(1, len(val_losses)+1), val_losses, marker='o', label=label)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss for Different Hyperparameters')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7e1c50-e90a-46b4-a366-712210cd484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Build encoder with best hyperparameters\n",
    "encoder = Encoder(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=128,     # BEST hidden size\n",
    "    num_heads=4,         # same as before\n",
    "    num_layers=2,        # BEST number of layers\n",
    "    intermediate_size=512,\n",
    "    max_len=512\n",
    ")\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "# Prepare your dataset (same as before)\n",
    "dataset = TextDataset(all_sentences, tokenizer, max_len=32)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Train\n",
    "print(\"Training BEST encoder\")\n",
    "train_losses, val_losses = train_bert(\n",
    "    model=encoder,\n",
    "    dataloader=dataloader,\n",
    "    tokenizer=tokenizer,\n",
    "    epochs=5,\n",
    "    lr=5e-4,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Save\n",
    "torch.save(encoder.state_dict(), \"pretrained_encoder_best.pt\")\n",
    "print(\"Saved best encoder to pretrained_encoder_best.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21400b10-1f07-4bca-868b-314cf6252bbd",
   "metadata": {},
   "source": [
    "# Generate All Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f1b1eb-bc15-4944-aae9-793014eea372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle, os\n",
    "from transformers import BertTokenizerFast\n",
    "from encoder import Encoder\n",
    "from preprocessing import downsample_word_vectors, make_delayed\n",
    "\n",
    "# Setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(\"/jet/home/cpestell/bert-base-uncased\")  # your local copy\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "encoder = Encoder(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=128,  # best hyperparameter you found!\n",
    "    num_heads=4,      # same as you trained\n",
    "    num_layers=2,     # best hyperparameter you found!\n",
    "    intermediate_size=512,  # same as training\n",
    "    max_len=512\n",
    ")\n",
    "\n",
    "encoder.load_state_dict(torch.load(\"pretrained_encoder_best.pt\", map_location=device))\n",
    "encoder.to(device)\n",
    "encoder.eval()\n",
    "\n",
    "# Load raw text\n",
    "data_path = \"/ocean/projects/mth240012p/shared/data\"\n",
    "with open(os.path.join(data_path, \"raw_text.pkl\"), \"rb\") as f:\n",
    "    raw_text = pickle.load(f)\n",
    "\n",
    "story_names = list(raw_text.keys())\n",
    "\n",
    "# Step 1: Generate encoder embeddings\n",
    "encoder_vectors = {}\n",
    "for story in story_names:\n",
    "    words = raw_text[story].data\n",
    "    inputs = tokenizer(words, padding=True, truncation=True, return_tensors=\"pt\", max_length=64)\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    token_type_ids = inputs[\"token_type_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden = encoder(input_ids, token_type_ids, attention_mask)\n",
    "\n",
    "    # Mean over tokens (seq_len)\n",
    "    sentence_embeddings = hidden.mean(dim=1).cpu().numpy()\n",
    "    encoder_vectors[story] = sentence_embeddings\n",
    "    # print(f\"{story}: Encoder shape = {encoder_vectors[story].shape}\")\n",
    "\n",
    "# Step 2: Downsample\n",
    "wordseqs = raw_text\n",
    "downsampled_encoder = downsample_word_vectors(story_names, encoder_vectors, wordseqs)\n",
    "\n",
    "# Step 3: Trim\n",
    "X_encoder_trimmed = {}\n",
    "for story in downsampled_encoder:\n",
    "    X_encoder_trimmed[story] = downsampled_encoder[story][5:-10, :]\n",
    "    # print(f\"Trimmed shape for {story}:\", X_encoder_trimmed[story].shape)\n",
    "\n",
    "# Step 4: Create lagged features\n",
    "X_encoder_lagged = {}\n",
    "for story in X_encoder_trimmed:\n",
    "    X_encoder_lagged[story] = make_delayed(X_encoder_trimmed[story], delays=range(1, 5))\n",
    "    # print(f\"Lagged shape for {story}:\", X_encoder_lagged[story].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4113160d-c96e-4372-87a6-b96d8ac4fdef",
   "metadata": {},
   "source": [
    "# Modelling and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127fcd73-3294-4de8-8ee6-7ddc00f85175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --------- Setup ---------\n",
    "story_names = list(raw_text.keys())\n",
    "train_stories, test_stories = train_test_split(story_names, test_size=0.2, random_state=42)\n",
    "\n",
    "subject2_path = os.path.join(data_path, \"subject2\")\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "\n",
    "\n",
    "for story in train_stories:\n",
    "    if story in X_encoder_lagged:\n",
    "        bold_path = os.path.join(subject2_path, f\"{story}.npy\")\n",
    "        if os.path.exists(bold_path):\n",
    "            Y = np.load(bold_path)\n",
    "            X = X_encoder_lagged[story]\n",
    "            min_len = min(X.shape[0], Y.shape[0])\n",
    "            X_train.append(X[:min_len])\n",
    "            Y_train.append(Y[:min_len])\n",
    "        else:\n",
    "            print(f\"Missing Y file for: {story}\")\n",
    "\n",
    "X_train = np.vstack(X_train)\n",
    "Y_train = np.vstack(Y_train)\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"Y_train:\", Y_train.shape)\n",
    "\n",
    "# --------- Clean NaNs ---------\n",
    "mask = ~np.isnan(Y_train).any(axis=1)\n",
    "X_train_clean = X_train[mask]\n",
    "Y_train_clean = Y_train[mask]\n",
    "\n",
    "# --------- Cross Validation ---------\n",
    "sample_size = int(0.1 * X_train_clean.shape[0])  # 20% of the data\n",
    "X_train_sub = X_train_clean[:sample_size]\n",
    "Y_train_sub = Y_train_small[:sample_size]\n",
    "alphas = np.logspace(-4, 4, 100)  # Example range for alpha\n",
    "ridge = Ridge()\n",
    "grid_search = GridSearchCV(ridge, param_grid={'alpha': alphas}, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_sub, Y_train_sub)\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "print(f\"\\nBest alpha found: {best_alpha}\")\n",
    "\n",
    "# --------- Train Model ---------\n",
    "ridge_best = Ridge(alpha=best_alpha)\n",
    "ridge_best.fit(X_train_clean, Y_train_small)\n",
    "\n",
    "# --------- Evaluate on Test Set ---------\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "for story in test_stories:\n",
    "    if story in X_encoder_lagged:\n",
    "        bold_path = os.path.join(subject2_path, f\"{story}.npy\")\n",
    "        if os.path.exists(bold_path):\n",
    "            Y = np.load(bold_path)\n",
    "            X = X_encoder_lagged[story]\n",
    "            min_len = min(X.shape[0], Y.shape[0])\n",
    "            X_test.append(X[:min_len])\n",
    "            Y_test.append(Y[:min_len])\n",
    "        else:\n",
    "            print(f\"Missing Y file for: {story}\")\n",
    "\n",
    "X_test = np.vstack(X_test)\n",
    "Y_test = np.vstack(Y_test)\n",
    "\n",
    "# Clean NaNs from test\n",
    "mask = ~np.isnan(Y_test).any(axis=1)\n",
    "X_test_clean = X_test[mask]\n",
    "Y_test_clean = Y_test[mask]\n",
    "\n",
    "# Predict\n",
    "Y_pred = ridge_best.predict(X_test_clean)\n",
    "\n",
    "# --------- Compute Correlation Coefficients (CCs) ---------\n",
    "def compute_voxel_ccs(y_true, y_pred):\n",
    "    ccs = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        try:\n",
    "            cc, _ = pearsonr(y_true[:, i], y_pred[:, i])\n",
    "        except:\n",
    "            cc = np.nan\n",
    "        ccs.append(cc)\n",
    "    return np.array(ccs)\n",
    "\n",
    "ccs_encoder = compute_voxel_ccs(Y_test_clean, Y_pred)\n",
    "\n",
    "print(f\"Mean CC: {np.mean(ccs_encoder):.4f}\")\n",
    "print(f\"Median CC: {np.median(ccs_encoder):.4f}\")\n",
    "print(f\"Top 1% CC: {np.percentile(ccs_encoder, 99):.4f}\")\n",
    "print(f\"Top 5% CC: {np.percentile(ccs_encoder, 95):.4f}\")\n",
    "\n",
    "# --------- Visualization ---------\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.histplot(ccs_encoder[~np.isnan(ccs_encoder)], bins=100, kde=True, color=\"#1f77b4\", edgecolor=\"black\", stat=\"count\")\n",
    "plt.axvline(np.mean(ccs_encoder), color='red', linestyle='--', label=f\"Mean: {np.mean(ccs_encoder):.3f}\")\n",
    "plt.axvline(np.median(ccs_encoder), color='green', linestyle='-.', label=f\"Median: {np.median(ccs_encoder):.3f}\")\n",
    "plt.title(\"Encoder Correlation Coefficient (CC) Distribution (5000 Voxels)\", fontsize=16, weight=\"bold\")\n",
    "plt.xlabel(\"Correlation Coefficient\", fontsize=13)\n",
    "plt.ylabel(\"Number of Voxels\", fontsize=13)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "save_dir = os.path.join(\"..\", \"results\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "plot_path = os.path.join(save_dir, \"cc_distribution_encoder_subject2_5000vox.png\")\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plot saved to: {plot_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_214",
   "language": "python",
   "name": "env_214"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
